<!DOCTYPE html>
<html>
<head>
    <!--Importing the highlight.js library styles -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
    <style>
        #contentDiv pre code.hljs {
            font-size: 1.125em; /* Adjust as needed */
            padding: 1em;
            box-sizing: border-box;
            white-space: pre-wrap;
        }

        #contentDiv p {
            font-size: 1.25em; /* Adjust this value as needed */
            margin-bottom: 10px;
            overflow-wrap: break-word;
            word-wrap: break-word;
        }

        #contentDiv {
            font-family: Inter, sans-serif;
            max-width: 1200px;
            margin: auto;
            box-sizing: border-box;
            overflow-x: auto;
            height: auto;
        }

        @media only screen and (max-width: 810px) {
            #contentDiv {
                font-family: Inter, sans-serif;
                max-width: 790px;
                margin: auto;
                box-sizing: border-box;
                overflow-x: auto;
                height: auto;
            }

            pre {
                white-space: pre-wrap;
                word-break: break-all;
            }
        }

        @media only screen and (max-width: 600px) {
            #contentDiv {
                padding: 0 10px;
                max-width: 580px;
                height: auto;
            }

            #contentDiv pre {
                white-space: pre-wrap;
                word-break: break-all;
            }
        }
    </style>
</head>
<body>
<div id="contentDiv">
    <h1>Поиск изображений с использованием Python и модели VisionEncoderDecoder</h1>
    <p>В нашем быстро развивающемся цифровом мире поиск изображений стал обыденным явлением. Однако, не всегда у нас
        есть возможность описать то, что мы ищем, используя ключевые слова. Было бы замечательно, если бы мы могли
        искать изображения, используя другие изображения в качестве исходных данных. Именно такой подход и
        предлагает Python библиотека Transformers, которую мы рассмотрим в этой статье.</p>
    <h2>Установка необходимых библиотек</h2>
    <p>Для начала нам необходимо установить нужные библиотеки. Мы будем использовать transformers и torch.
        Установить их можно с помощью команды pip:</p>
    <pre><code class="hljs python">
    !pip install -qU transformers torch
</code></pre>

    <h2>Импорт библиотек и загрузка модели</h2>
    <p>После установки библиотек, нам нужно их импортировать и загрузить предварительно обученную модель. Здесь мы
        используем VisionEncoderDecoderModel, которая является обобщенной моделью для задач кодирования и
        декодирования визуальных данных.</p>
    <pre><code class="hljs python">
    from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer
import torch
from PIL import Image

model = VisionEncoderDecoderModel.from_pretrained("tuman/vit-rugpt2-image-captioning")
feature_extractor = ViTFeatureExtractor.from_pretrained("tuman/vit-rugpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("tuman/vit-rugpt2-image-captioning")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

max_length = 16
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams}
</code></pre>
    <h2>Предсказание подписи к изображению</h2>
    <p>Теперь, когда мы импортировали необходимые библиотеки и загрузили модель, мы можем предсказать подпись к
        изображению. Это поможет нам связать изображение с текстовым описанием.</p>
    <pre><code class="hljs python">
    rom io import BytesIO

def predict_caption(image_bytes):
images = []
i_image = Image.open(BytesIO(image_bytes))

images.append(i_image)

pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values
pixel_values = pixel_values.to(device)

output_ids = model.generate(pixel_values, **gen_kwargs)

preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

preds = [pred.strip() for pred in preds]
return preds
</code></pre>

    <h2>Загрузка изображения</h2>
    <p>После того, как мы определили функцию для предсказания подписи, нам нужно загрузить изображение. Мы будем
        использовать функциональность Google Colab для загрузки файла, но вы можете использовать любой другой подход
        для получения изображения.</p>
    <pre><code class="hljs python">
from google.colab import files
    uploaded = files.upload()

if uploaded:
# Get the first uploaded file (as we set multiple=False)
uploaded_image = list(uploaded.values())[0]
capt= predict_caption(uploaded_image)
new_image = Image.open(BytesIO(uploaded_image))
# Display the new image
print(capt)
print("New Image:")
display(new_image)
</code></pre>

    <h2>Заключение</h2>
    <p>В этой статье мы рассмотрели, как можно использовать Python и библиотеку transformers для поиска изображений.
        Мы использовали модель VisionEncoderDecoder для генерации текстового описания изображения, которое затем
        можно использовать для поиска соответствующих изображений. Такой подход может быть полезен в многочисленных
        приложениях, от поиска визуального контента до разработки систем компьютерного зрения.</p>

</div>

<script type="text/javascript">
    setTimeout(function () {
        var myDiv = document.getElementById('contentDiv');
        if (myDiv) {
            myDiv.style.height = 'auto';
            var script = document.createElement('script');
            script.src = "//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js";
            script.onload = function () {
                hljs.highlightAll();
            };
            document.body.appendChild(script);
        }
    }, 0);
</script>
</body>
</html>